https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html

> bidirectional GRU -> zwracanie sumy outputow
> SOS, EOS, (PAD) TOKENS (26 + 3 = 29 klas literek)
> attention after decoder
> padding po dlugosci labeli i outputow

> przesuniecie glupiego paddingu
> zerk
> pooling gotta go
> do smth with hidden_forward?
> dropout -= 0.05?
> bring back global attention?
> outputs of variable lengths?
> more GRU layers
> zgapic optimiser i scheduler z v8